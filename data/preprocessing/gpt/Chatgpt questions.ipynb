{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c04e4136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai\n",
    "#!pip install python-dotenv\n",
    "#!pip install tenacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f442dd3",
   "metadata": {},
   "source": [
    "#### Instructions to save openai API key\n",
    "\n",
    "1. Create a .env file in the same folder as your script.\n",
    "2. The .env file should contain the following:\n",
    "- OPENAI_API_KEY = \"exampleapikey123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfce05a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import pandas as pd\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff (to overcome rate limit)\n",
    "\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95811708",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d377bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = pd.read_csv(\"~/Documents/Work/dsaid-hackathon23-illuminati/data/preprocessing/website/MTI/MTI_speeches_PQs_scraped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05e4e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exponential backoff decorator\n",
    "@retry(wait=wait_random_exponential(min=10, max=80), stop=stop_after_attempt(10))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.ChatCompletion.create(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fcf4305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to get returns from gpt\n",
    "def get_message_completion(messages, \n",
    "                     model=\"gpt-3.5-turbo\", \n",
    "                     temperature=0, \n",
    "                     max_tokens=500,\n",
    "                     num_pairings=100\n",
    "                    ):\n",
    "\n",
    "    response = completion_with_backoff(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "        max_tokens=max_tokens, # the maximum number of tokens the model can ouptut \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d6027f",
   "metadata": {},
   "source": [
    "### Step 1. Summarise relevant lines\n",
    "\n",
    "This is to create a more focused corpus out of documents that contain other unrelated info. \n",
    "\n",
    "Note: This section will take some time as we have deliberately slowed down the process to overcome Openai's rate limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1a2aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37380339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Speech by Minister Gan Kim Yong at the Joint Segment on the Singapore Green Plan Committee of Supply Debate 2022' was not summarised. Error message: RetryError[<Future at 0x203330a5d90 state=finished raised InvalidRequestError>]'\n",
      "'Speech by Second Minister for Trade and Industry Tan See Leng at Ministry of Trade and Industry (MTI)'s Committee of Supply Debate 2023' was not summarised. Error message: RetryError[<Future at 0x20333094fa0 state=finished raised InvalidRequestError>]'\n",
      "'Ministerial Statement by Second Minister for Trade and Industry Dr Tan See Leng for Parliament Sitting on 4 April 2022' was not summarised. Error message: RetryError[<Future at 0x203331c2790 state=finished raised InvalidRequestError>]'\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(speeches['Content'])):\n",
    "    \n",
    "    try:\n",
    "        #Set messages for chatgpt\n",
    "        messages =  [  \n",
    "        {'role':'system', \n",
    "         'content':f\"\"\"Please refer to the speech provided: {speeches['Content'][i].strip()}\"\"\"},    \n",
    "        {'role':'user', \n",
    "         'content':f\"\"\"Summarise the most relevant lines related to solar energy or solar panels. \\\n",
    "         Make sure to retain key statistics or figures.\"\"\"},  \n",
    "        ] \n",
    "        \n",
    "        #Obtain gpt response\n",
    "        response = get_message_completion(messages, max_tokens = 500)\n",
    "        \n",
    "        #Save to dictionary\n",
    "        extracts[speeches['Title'][i]] = response\n",
    "    except Exception as e:\n",
    "        print(f\"'{speeches['Title'][i]}' was not summarised. Error message: {e}'\")\n",
    "    \n",
    "    #time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b66c5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4ce8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracts_df = pd.DataFrame(\n",
    "    [(k, val) for k, val in extracts.items()], \n",
    "    columns=['Title', 'Summary']\n",
    ")\n",
    "\n",
    "extracts_df.to_csv(\"MTI_speeches_PQs_summaries_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a84c1",
   "metadata": {},
   "source": [
    "### Step 2. Generate question and answer pairings from summaries\n",
    "\n",
    "Note: This section will take some time as we have deliberately slowed down the process to overcome Openai's rate limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65cd28aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "qna = {}\n",
    "num_qna = 10 #Set number of Q&As"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52a9092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, s in extracts.items():\n",
    "    \n",
    "    try:\n",
    "        #Set messages for chatgpt\n",
    "        messages =  [  \n",
    "        {'role':'system', \n",
    "         'content':f\"\"\"Please refer to the content provided: {s.strip()}\"\"\"},    \n",
    "        {'role':'user', \n",
    "         'content':f\"\"\"# Create a JSON of {num_qna} pairs of questions and answers based on this summary. \\\n",
    "         The key value pairs should be the question and answer.\"\"\"},  \n",
    "        ] \n",
    "        \n",
    "        #Obtain gpt response\n",
    "        response = get_message_completion(messages, max_tokens = 1000)\n",
    "        \n",
    "        #Convert response to JSON and then dictionary\n",
    "        qna_dict = dict(json.loads(response))\n",
    "        \n",
    "        #Create list of tuples based on q&a pairings and save to dictionary\n",
    "        qna[t] = [(qna_dict[k]['question'], qna_dict[k]['answer']) for k in qna_dict.keys()] \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Did not generate Q&As for'{t}'. Error message: {e}'\")\n",
    "    \n",
    "    #time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca36e3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "800bf611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create df and save to csv\n",
    "qna_df = pd.DataFrame(qna)\n",
    "qna_melt = pd.melt(qna_df, value_vars = [k for k in qna.keys()], var_name = \"Title\")\n",
    "qna_melt['Questions'], qna_melt['answers'] = zip(*qna_melt['value'])\n",
    "qna_melt = qna_melt.drop('value', axis = 1)\n",
    "\n",
    "\n",
    "qna_melt.to_csv(\"MTI_speeches_PQ_qna_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c74ba",
   "metadata": {},
   "source": [
    "### To-do / issues:\n",
    "\n",
    "1. Quite a bit of duplicate content. Should we remove those? Possible to remove using Chatgpt/langchain?\n",
    "2. Rate limit of 3/min and max tokens (4097) is very low and limiting. Tried exponential backoff method + time.sleep but script runs very slowly as a result.\n",
    "3. InvalidREquestError for 3 documents - max tokens exceeded."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
